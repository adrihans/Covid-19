{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Embeddings techniques\n",
    "\n",
    "Documentations:\n",
    "\n",
    "https://stackoverflow.com/questions/48362530/doc2vec-infer-vector-keeps-giving-different-result-everytime-on-a-particular-tra\n",
    "\n",
    "https://medium.com/@japneet121/document-vectorization-301b06a041\n",
    "\n",
    "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d\n",
    "\n",
    "https://towardsdatascience.com/super-easy-way-to-get-sentence-embedding-using-fasttext-in-python-a70f34ac5b7c\n",
    "\n",
    "https://towardsdatascience.com/fse-2b1ffa791cf9\n",
    "\n",
    "https://towardsdatascience.com/building-a-sentence-embedding-index-with-fasttext-and-bm25-f07e7148d240\n",
    "\n",
    "https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1\n",
    "\n",
    "https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "\n",
    "https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "https://medium.com/analytics-vidhya/combining-word-embeddings-to-form-document-embeddings-9135a66ae0f\n",
    "\n",
    "https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "https://stackoverflow.com/questions/50373248/how-to-generate-word2vec-vectors-in-python\n",
    "\n",
    "\n",
    "**Easiest:**\n",
    "https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "model=Doc2Vec(vector_size=200)\n",
    "corpus = [\n",
    "     'This is the first document',\n",
    "     'This is the second second document',\n",
    "     'And the third one',]\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "\n",
    "#model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model.build_vocab(documents)\n",
    "\n",
    "model.train(documents,total_examples=model.corpus_count, epochs=2)\n",
    "\n",
    "\n",
    "#len([model.infer_vector(s) for s in ([\"second\", \"document\"],[\"third\",\"document\"])])\n",
    "\n",
    "model.infer_vector([\"second\", \"document\"]).shape\n",
    "\n",
    "\n",
    "\n",
    "tfidf.vocabulary_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### FastText:\n",
    "\n",
    "import fasttext\n",
    "model = fasttext.load_model(\"embeddings_models/wiki-news-300d-1M-subword/wiki-news-300d-1M-subword.vec\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import fasttext.util\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "#ft = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "fname=\"embeddings_models/wiki-news-300d-1M-subword/wiki-news-300d-1M-subword.vec\"\n",
    "\n",
    "model=load_vectors(fname)\n",
    "\n",
    "[model.get_word_vector(x) for x in [\"asparagus\", \"pidgey\", \"yellow\"]]\n",
    "\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/word-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tfidf.get_feature_names()[114035]\n",
    "\n",
    "vector_test=tfidf.transform(['vp3'])\n",
    "\n",
    "vocabulary=tfidf.get_feature_names()[114035]\n",
    "\n",
    "embedding_vocab_vector=[ft.get_word_vector(word) for word in tqdm(tfidf.get_feature_names())]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Si on utilise un dataset fasttext:\n",
    "@inproceedings{mikolov2018advances,\n",
    "  title={Advances in Pre-Training Distributed Word Representations},\n",
    "  author={Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},\n",
    "  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},\n",
    "  year={2018}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
